{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weixincmu/community/blob/master/Google_ADK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh6U7K24_KRt"
      },
      "outputs": [],
      "source": [
        "!pip install google-adk -q\n",
        "!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install litellm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJu5p-9XHjDM",
        "outputId": "0d3dfb51-b5db-460d-9843-f1c74168a19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/7.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n"
      ],
      "metadata": {
        "id": "4u5qfUU7_n7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "GEMINI=getpass(\"Enter your GEMINI API KEY: \")\n",
        "os.environ[\"GOOGLE_API_KEY\"]=GEMINI\n",
        "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4hI0NtOAX4b",
        "outputId": "de5aa4c5-d79b-4aad-b538-4062d037ee2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GEMINI API KEY: ··········\n",
            "Google API Key set: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= \"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "oyMzT0FXAwY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Your Tool"
      ],
      "metadata": {
        "id": "Une2ubopDazY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather(city: str) -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city (e.g., \"Mumbai\",\"Chennai\",\"Delhi\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes a 'report' key with weather details.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    # Best Practice: Log tool execution for easier debugging\n",
        "    print(f\"--- Tool: get_weather called for city: {city} ---\")\n",
        "\n",
        "    city_normalized = city.lower().replace(\" \", \"\")  # Basic input normalization\n",
        "\n",
        "    mock_weather_db = {\n",
        "        \"delhi\": {\"status\": \"success\", \"report\": \"The weather in Delhi is sunny with a temperature of 35°C.\"},\n",
        "        \"mumbai\": {\"status\": \"success\", \"report\": \"It's humid in Mumbai with a temperature of 30°C.\"},\n",
        "        \"bangalore\": {\"status\": \"success\", \"report\": \"Bangalore is experiencing light showers and a temperature of 22°C.\"},\n",
        "        \"kolkata\": {\"status\": \"success\", \"report\": \"Kolkata is partly cloudy with a temperature of 29°C.\"},\n",
        "        \"chennai\": {\"status\": \"success\", \"report\": \"It's hot and humid in Chennai with a temperature of 33°C.\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        return mock_weather_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
        "\n",
        "\n",
        "# Example usage\n",
        "print(get_weather(\"Mumbai\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpASw5C6BQ7d",
        "outputId": "566c995c-2057-4610-9565-94ddff521a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: Mumbai ---\n",
            "{'status': 'success', 'report': \"It's humid in Mumbai with a temperature of 30°C.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Agent"
      ],
      "metadata": {
        "id": "l3iM5bQfDmS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AGENT_MODEL=model\n",
        "weather_agent=Agent(\n",
        "    name=\"weather_agent_v1\",\n",
        "    model=AGENT_MODEL,\n",
        "    description=\"Provides weather information for specific cities.\",\n",
        "    instruction=\"You are a helpful weather assistant. Your primary goal is to provide current weather reports. \"\n",
        "                \"When the user asks for the weather in a specific city, \"\n",
        "                \"you MUST use the 'get_weather' tool to find the information. \"\n",
        "                \"Analyze the tool's response: if the status is 'error', inform the user politely about the error message. \"\n",
        "                \"If the status is 'success', present the weather 'report' clearly and concisely to the user. \"\n",
        "                \"Only use the tool when a city is mentioned for a weather request.\",\n",
        "    tools=[get_weather],\n",
        "\n",
        ")\n",
        "\n",
        "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YZc7PNTCv6X",
        "outputId": "2290cc35-e959-45ea-84a3-e60a99b6f744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Runner and Session Services"
      ],
      "metadata": {
        "id": "huFGBerNEKNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Session Service and Runner\n",
        "# ---Session Management ---\n",
        "# Key Concept: SessionService stores conversation history & state.\n",
        "# InMemorySessionService is  simple, non-persistent storage for this tutorial.\n",
        "\n",
        "session_service=InMemorySessionService()\n",
        "\n",
        "# Define constants for identifying the interaction context\n",
        "APP_NAME=\"weathertutorial_app\"\n",
        "USER_ID=\"user_1\"\n",
        "SESSION_ID=\"session_001\"\n",
        "\n",
        "# Create the specific session where the conversation will happen\n",
        "session=session_service.create_session(\n",
        "    app_name=APP_NAME,\n",
        "    user_id=USER_ID,\n",
        "    session_id=SESSION_ID,\n",
        ")\n",
        "\n",
        "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "\n",
        "# ---Runner ---\n",
        "# Key Concept: Runner orchestrates the agent execution loop.\n",
        "\n",
        "runner=Runner(\n",
        "    agent=weather_agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "print(f\"Runner created for agent '{runner.agent.name}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoQlsWCcEGfX",
        "outputId": "02f0dab1-139a-47f5-ad7a-17c11869fc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session created: App='weathertutorial_app', User='user_1', Session='session_001'\n",
            "Runner created for agent 'weather_agent_v1'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interact with the Agent"
      ],
      "metadata": {
        "id": "9VbZelmEFggT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Agent Interaction Function\n",
        "import asyncio\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "async def call_agent_async(query: str):\n",
        "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
        "  print(f\"\\n>>> User Query: {query}\")\n",
        "\n",
        "  # Prepare the user's message in ADK format\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
        "\n",
        "  # Key Concept: run_async executes the agent logic and yields Events.\n",
        "  # We iterate through events to find the final answer.\n",
        "  async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "      # You can uncomment the line below to see *all* events during execution\n",
        "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
        "\n",
        "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "             # Assuming text response in the first part\n",
        "             final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
        "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
        "          # Add more checks here if needed (e.g., specific error codes)\n",
        "          break # Stop processing events once the final response is found\n",
        "\n",
        "  print(f\"<<< Agent Response: {final_response_text}\")"
      ],
      "metadata": {
        "id": "szEq-dCxFduN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Conversation"
      ],
      "metadata": {
        "id": "1TMRj3YCF5jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the Initial Conversation\n",
        "\n",
        "# # We need an async function to await our interaction helper\n",
        "# async def run_conversation():\n",
        "#     await call_agent_async(\"What is the weather like in Mumbai\")\n",
        "#     await call_agent_async(\"How about Delhi?\") # Expecting the tool's error message\n",
        "#     await call_agent_async(\"Tell me the weather in CHennai\")\n",
        "\n",
        "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
        "await run_conversation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdiQMTzNF23I",
        "outputId": "a493d093-94c7-47ba-cdb0-cee3d222f1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> User Query: What is the weather like in Mumbai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: Mumbai ---\n",
            "<<< Agent Response: It's humid in Mumbai with a temperature of 30°C.\n",
            "\n",
            "\n",
            ">>> User Query: How about Delhi?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: Delhi ---\n",
            "<<< Agent Response: The weather in Delhi is sunny with a temperature of 35°C.\n",
            "\n",
            "\n",
            ">>> User Query: Tell me the weather in CHennai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: Chennai ---\n",
            "<<< Agent Response: It's hot and humid in Chennai with a temperature of 33°C.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\".\n",
        "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
        "MODEL_CLAUDE_SONNET = \"anthropic/claude-3-sonnet-20240229\"\n",
        "print(\"\\nEnvironment configured.\")\n"
      ],
      "metadata": {
        "id": "eU6fxf0ZGESm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}